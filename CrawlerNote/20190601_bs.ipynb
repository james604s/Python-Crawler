{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup 重要功能\n",
    "* find()\n",
    "* find_all()\n",
    "* .text()\n",
    "* .stripped_strings()\n",
    "* 熟悉前段網頁元件的人可以使用 CSS Select .select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h4 class=\"card-title\">\n",
      "<a href=\"http://www.pycone.com/blogs#pablo\">Mac使用者</a>\n",
      "</h4>\n",
      "<h4 class=\"card-title\">\n",
      "<a href=\"http://www.pycone.com/blogs#pablo\">Mac使用者</a>\n",
      "</h4>\n",
      "Mac使用者\n",
      "Mac使用者\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "Mac使用者\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "<p class=\"card-description\" id=\"mac-p\">\n",
      "                                    在Mac環境下安裝Python與Sublime Text3<a data-foo=\"mac-foo\" href=\"http://www.pycone.com/blogs/mac-python-environment\"> <br/>Read More </a>\n",
      "</p>\n",
      "<a data-foo=\"mac-foo\" href=\"http://www.pycone.com/blogs/mac-python-environment\"> <br/>Read More </a>\n",
      "['開發環境設定', 'Mac使用者', '在Mac環境下安裝Python與Sublime Text3', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(1) 前言', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(2) 套件安裝與啟動網頁爬蟲', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(3) 解構並擷取網頁資料', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(4) 擷取資料及下載圖片', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(5) 資料分析及展示', 'Read More']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def main():\n",
    "    resp = requests.get('http://jwlin.github.io/py-scraping-analysis-book/ch2/blog/blog.html')\n",
    "    soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "\n",
    "    # 取得第一篇 blog (h4)\n",
    "    print(soup.find('h4'))\n",
    "    print(soup.h4)  # 與上一行相等\n",
    "\n",
    "    # 取得第一篇 blog 主標題\n",
    "    print(soup.h4.a.text)\n",
    "\n",
    "    # 取得所有 blog 主標題, 使用 tag\n",
    "    main_titles = soup.find_all('h4')\n",
    "    for title in main_titles:\n",
    "        print(title.a.text)\n",
    "\n",
    "    # 取得所有 blog 主標題, 使用 class\n",
    "    # 以下寫法皆相同:\n",
    "    # soup.find_all('h4', 'card-title')\n",
    "    # soup.find_all('h4', {'class': 'card-title'})\n",
    "    # soup.find_all('h4', class_='card-title')\n",
    "    main_titles = soup.find_all('h4', 'card-title')\n",
    "    for title in main_titles:\n",
    "        print(title.a.text)\n",
    "\n",
    "    # 使用 key=value 取得元件\n",
    "    print(soup.find(id='mac-p'))\n",
    "\n",
    "    # 當 key 含特殊字元時, 使用 dict 取得元件\n",
    "    # print(soup.find(data-foo='mac-foo'))  # 會導致 SyntaxError\n",
    "    print(soup.find('', {'data-foo': 'mac-foo'}))\n",
    "\n",
    "    # 取得各篇 blog 的所有文字\n",
    "    divs = soup.find_all('div', 'content')\n",
    "    for div in divs:\n",
    "        # 方法一, 使用 text (會包含許多換行符號)\n",
    "        #print(div.text)\n",
    "        # 方法二, 使用 tag 定位\n",
    "        #print(div.h6.text.strip(), div.h4.a.text.strip(), div.p.text.strip())\n",
    "        # 方法三, 使用 .stripped_strings\n",
    "        print([s for s in div.stripped_strings])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find( ) 接受標籤名稱作為引數，回傳第一個找到的HTML元件，soup.find('tagname')。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h4 class=\"card-title\">\n",
      "<a href=\"http://www.pycone.com/blogs#pablo\">Mac使用者</a>\n",
      "</h4>\n",
      "<h4 class=\"card-title\">\n",
      "<a href=\"http://www.pycone.com/blogs#pablo\">Mac使用者</a>\n",
      "</h4>\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get('http://jwlin.github.io/py-scraping-analysis-book/ch2/blog/blog.html')\n",
    "soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "# 取得第一篇 blog (h4)\n",
    "print(soup.find('h4'))\n",
    "print(soup.h4)  # 與上一行相等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find_all( ) 回傳所有符合條件的HTML元件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mac使用者\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n"
     ]
    }
   ],
   "source": [
    "main_titles = soup.find_all('h4')\n",
    "for title in main_titles:\n",
    "    print(title.a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mac使用者\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n",
      "給初學者的 Python 網頁爬蟲與資料分析\n"
     ]
    }
   ],
   "source": [
    "    # 取得所有 blog 主標題, 使用 class\n",
    "    # 以下寫法皆相同:\n",
    "    # soup.find_all('h4', 'card-title')\n",
    "    # soup.find_all('h4', {'class': 'card-title'})\n",
    "    # soup.find_all('h4', class_='card-title')\n",
    "    main_titles = soup.find_all('h4', 'card-title')\n",
    "    for title in main_titles:\n",
    "        print(title.a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"card-description\" id=\"mac-p\">\n",
      "                                    在Mac環境下安裝Python與Sublime Text3<a data-foo=\"mac-foo\" href=\"http://www.pycone.com/blogs/mac-python-environment\"> <br/>Read More </a>\n",
      "</p>\n",
      "<a data-foo=\"mac-foo\" href=\"http://www.pycone.com/blogs/mac-python-environment\"> <br/>Read More </a>\n"
     ]
    }
   ],
   "source": [
    "# 使用 key=value 取得元件\n",
    "    print(soup.find(id='mac-p'))\n",
    "\n",
    "    # 當 key 含特殊字元時, 使用 dict 取得元件\n",
    "    # print(soup.find(data-foo='mac-foo'))  # 會導致 SyntaxError\n",
    "    print(soup.find('', {'data-foo': 'mac-foo'}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['開發環境設定', 'Mac使用者', '在Mac環境下安裝Python與Sublime Text3', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(1) 前言', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(2) 套件安裝與啟動網頁爬蟲', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(3) 解構並擷取網頁資料', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(4) 擷取資料及下載圖片', 'Read More']\n",
      "['資料科學', '給初學者的 Python 網頁爬蟲與資料分析', '(5) 資料分析及展示', 'Read More']\n"
     ]
    }
   ],
   "source": [
    "    # 取得各篇 blog 的所有文字\n",
    "    divs = soup.find_all('div', 'content')\n",
    "    for div in divs:\n",
    "        # 方法一, 使用 text (會包含許多換行符號)\n",
    "        #print(div.text)\n",
    "        # 方法二, 使用 tag 定位\n",
    "        #print(div.h6.text.strip(), div.h4.a.text.strip(), div.p.text.strip())\n",
    "        # 方法三, 使用 .stripped_strings\n",
    "        print([s for s in div.stripped_strings])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
